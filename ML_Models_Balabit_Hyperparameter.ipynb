{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Models - Balabit Hyperparameter.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yY96zsPUlIDK",
        "1PfcCJ_LVg55"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylansoemitro/MouseBiometrics/blob/main/ML_Models_Balabit_Hyperparameter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWVFVmik1qv4"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4bc1mGz3bUb"
      },
      "source": [
        "Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbqGPoLa-E5w",
        "outputId": "7759f929-f4d4-445f-c969-44c2c3412ac1"
      },
      "source": [
        "!pip install -U scikit-learn\n",
        "!pip install -U imbalanced-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.2.0\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.8.0-py3-none-any.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (0.24.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.19.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (2.2.0)\n",
            "Installing collected packages: imbalanced-learn\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.4.3\n",
            "    Uninstalling imbalanced-learn-0.4.3:\n",
            "      Successfully uninstalled imbalanced-learn-0.4.3\n",
            "Successfully installed imbalanced-learn-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8-LCImPsE5a",
        "outputId": "4228c3cb-4c6a-47ff-ba06-e340648fc065"
      },
      "source": [
        "!pip install -U collinearity"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting collinearity\n",
            "  Downloading collinearity-0.6.1.tar.gz (5.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from collinearity) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from collinearity) (0.24.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->collinearity) (2.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->collinearity) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->collinearity) (1.4.1)\n",
            "Building wheels for collected packages: collinearity\n",
            "  Building wheel for collinearity (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for collinearity: filename=collinearity-0.6.1-py3-none-any.whl size=4459 sha256=b17069cdfa44b6553a5825a3431104a3293b97e06e1d6bb7436de20d87897151\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/bf/74/0a475ad9095545c56fe02d678ccd38739baa81513e877d91ca\n",
            "Successfully built collinearity\n",
            "Installing collected packages: collinearity\n",
            "Successfully installed collinearity-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEybiaiUFgjd"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R2jvrRr2A9U",
        "outputId": "56062886-7806-4478-fefd-abb4f1fb84a6"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "from pathlib import Path\n",
        "\n",
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2\n",
        "from collinearity import SelectNonCollinear\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxdG0KeE3ebj"
      },
      "source": [
        "Link google drive to access datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fusZH1Tl3aJw",
        "outputId": "681bf4a3-64ae-42f3-85d9-be02c947e981"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# enable use of google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# set the working directory\n",
        "% cd /content/drive/My Drive/Colab Notebooks/2021 Mouse Biometrics Internship/\n",
        "\n",
        "# link to project directory\n",
        "project_folder = \"/content/drive/My Drive/Colab Notebooks/2021 Mouse Biometrics Internship/\"\n",
        "\n",
        "# clean of previous directories from sys\n",
        "while(True):\n",
        "  try:\n",
        "    sys.path.remove(project_folder)\n",
        "  except ValueError:\n",
        "    break\n",
        "# append new path\n",
        "sys.path.append(project_folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1k64oZq9UGrPEz-MMYl1rpRwF_LjXSCrB/2021 Mouse Biometrics Internship\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgdSylc91tgz"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87jSRTuAjXPd"
      },
      "source": [
        "## BB-MAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QbaJmX3DPfe"
      },
      "source": [
        "bb_mas_path = \"Data/PrepedCSVs/BB-MAS.csv\"\n",
        "bb_mas_df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXPEECYtMslU"
      },
      "source": [
        "if bb_mas_df.size == 0:\n",
        "  bb_mas_df = pd.read_csv(bb_mas_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sv0ocEZvhji"
      },
      "source": [
        "## TWOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMTG6J00DN3f"
      },
      "source": [
        "twos_df = pd.DataFrame()\n",
        "twos_path = \"Data/PrepedCSVs/TWOS.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJi2fZAIG2Hv"
      },
      "source": [
        "if twos_df.size == 0:\n",
        "  twos_df = pd.read_csv(twos_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lChf50AzvmbR"
      },
      "source": [
        "## Balabit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaXnXH92DMFr"
      },
      "source": [
        "balabit_df = pd.DataFrame()\n",
        "balabit_path = \"Data/PrepedCSVs/Balabit.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSAx7ePy7nwA"
      },
      "source": [
        "if balabit_df.size == 0:\n",
        "  balabit_df = pd.read_csv(balabit_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgmu_BrE1uBg"
      },
      "source": [
        "# Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY96zsPUlIDK"
      },
      "source": [
        "## Window generator functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIq7Ek1uP2dw"
      },
      "source": [
        "def time_window(dataframe, delta_time, shift=None, drop_remainder=False):\n",
        "  if len(dataframe) == 0:\n",
        "    return\n",
        "  p0 = dataframe.index[0]\n",
        "  while True:\n",
        "    p = p0\n",
        "    while dataframe[\"time\"][p] < delta_time + dataframe[\"time\"][p0]:\n",
        "      p += 1\n",
        "      if p == dataframe.index[-1]:\n",
        "        if not drop_remainder:\n",
        "          yield dataframe.loc[p0:]\n",
        "        return\n",
        "    \n",
        "    yield dataframe.loc[p0:p]\n",
        "    if shift is None: p0 = p\n",
        "    else: \n",
        "      while dataframe[\"time\"][p0] < shift + dataframe[\"time\"][p]:\n",
        "        p0 += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PfcCJ_LVg55"
      },
      "source": [
        "## Feature Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9a7Yu0UbAo1"
      },
      "source": [
        "# returns the total time in seconds between the interval or the delta times between each mouse move\n",
        "def get_elapsed_time(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  return user_data[\"time\"][end] - user_data[\"time\"][start]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K49OJwarbbd7"
      },
      "source": [
        "# returns distance in the x direction\n",
        "def get_x_distance(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  return user_data[\"px\"][end] - user_data[\"px\"][start]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJNWvQTIbdtM"
      },
      "source": [
        "# returns distance in the y direction\n",
        "def get_y_distance(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  return user_data[\"py\"][end] - user_data[\"py\"][start]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5jX9BirgQG7"
      },
      "source": [
        "# returns distance \n",
        "def get_euclidean_distance(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  xdist = get_x_distance(user_data, start, end)\n",
        "  ydist = get_y_distance(user_data, start, end)\n",
        "  edist = np.sqrt(np.square(xdist) + np.square(ydist))\n",
        "  return edist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF3ysexChger"
      },
      "source": [
        "def get_manhattan_distance(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  return get_x_distance(user_data, start, end) + get_y_distance(user_data, start, end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MK5WCoPbf-E"
      },
      "source": [
        "# returns velocity in the x direction\n",
        "def get_x_velocity(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  if get_elapsed_time(user_data, start, end) == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return get_x_distance(user_data, start, end) / get_elapsed_time(user_data, start, end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2FsVdj2bhoD"
      },
      "source": [
        "# returns velocity in the y direction\n",
        "def get_y_velocity(user_data, start, end):\n",
        "    if end > user_data.index[-1]:\n",
        "      return 0\n",
        "    if get_elapsed_time(user_data, start, end) == 0:\n",
        "      return 0\n",
        "    else:\n",
        "      return get_y_distance(user_data, start, end) / get_elapsed_time(user_data, start, end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7PlcDtzbjuE"
      },
      "source": [
        "# return speed\n",
        "def get_speed(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  if get_elapsed_time(user_data, start, end) == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return get_euclidean_distance(user_data, start, end) / get_elapsed_time(user_data, start, end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wccTxKJocGdA"
      },
      "source": [
        "# TODO: verify accuracy\n",
        "# return angular velocity\n",
        "def get_angular_velocity(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  time = get_elapsed_time(user_data, start, end)\n",
        "  point1 = (user_data[\"px\"][start], user_data[\"py\"][start])\n",
        "  point2 = (user_data[\"px\"][end], user_data[\"py\"][end])\n",
        "  ang1 = np.arctan2(*point1[::-1])\n",
        "  ang2 = np.arctan2(*point2[::-1])\n",
        "  ang_between = np.rad2deg((ang1 - ang2) % (2 * np.pi))\n",
        "  if time == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return ang_between / time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awGMZ2CjdNr6"
      },
      "source": [
        "# returns linear acceleration\n",
        "def get_acceleration(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  if end - start < 3:\n",
        "    return 0\n",
        "  delta_time = get_elapsed_time(user_data, start, end)\n",
        "  start_speed = get_speed(user_data, start, start+1)\n",
        "  end_speed = get_speed(user_data, end-1, end)\n",
        "  if delta_time == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return (end_speed - start_speed) / delta_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wNYg4LrilCl"
      },
      "source": [
        "def get_jerk(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  if end - start < 4:\n",
        "    return 0\n",
        "  delta_time = get_elapsed_time(user_data, start, end)\n",
        "  start_acceleration = get_acceleration(user_data, start, start+3)\n",
        "  end_acceleration = get_acceleration(user_data, end-3, end)\n",
        "  if delta_time == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    jerk = (end_acceleration - start_acceleration) / delta_time\n",
        "    return jerk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ5PzGHngAk6"
      },
      "source": [
        "# TODO: verify accuracy\n",
        "# returns curvature\n",
        "def get_curvature(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  dist = get_euclidean_distance(user_data, start, end)\n",
        "  point1 = (user_data[\"px\"][start], user_data[\"py\"][start])\n",
        "  point2 = (user_data[\"px\"][end], user_data[\"py\"][end])\n",
        "  ang1 = np.arctan2(*point1[::-1])\n",
        "  ang2 = np.arctan2(*point2[::-1])\n",
        "  ang_between = np.rad2deg((ang1 - ang2) % (2 * np.pi))\n",
        "\n",
        "  curv = ang_between / dist\n",
        "  if np.isnan(curv) or np.isinf(curv):\n",
        "      return 0\n",
        "  return curv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PefWmygfisWq"
      },
      "source": [
        "def get_curvature_change(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  if end - start < 3:\n",
        "    return 0\n",
        "  start_curv = get_curvature(user_data, start, start+1)\n",
        "  end_curv = get_curvature(user_data, end-1, end)\n",
        "  dist = get_euclidean_distance(user_data, start, end)\n",
        "  \n",
        "  dcurv = (end_curv - start_curv) / dist\n",
        "  if np.isnan(dcurv) or np.isinf(dcurv):\n",
        "      return 0\n",
        "  return dcurv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m2Q--Yvn4b6"
      },
      "source": [
        "def get_critical_points(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  if abs(get_curvature(user_data, start, end))>np.pi/10:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nlM4ljflML2"
      },
      "source": [
        "def get_direction(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  ydist = get_y_distance(user_data, start, end)\n",
        "  if ydist == 0:\n",
        "    return 0;\n",
        "  xdist = get_x_distance(user_data, start, end)\n",
        "  if xdist == 0:\n",
        "    return np.pi / 2\n",
        "  return np.arctan(ydist/xdist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIxOWNrnBPcO"
      },
      "source": [
        "def get_angle_of_curvature(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  d1 = get_euclidean_distance(user_data, start+1, start)\n",
        "  d3 = get_euclidean_distance(user_data, start+1, start+2)\n",
        "  d2 = get_euclidean_distance(user_data, start, start+2)\n",
        "  numerator = np.square(d1)+np.square(d2)-np.square(d3)\n",
        "\n",
        "  denominator = 2*get_euclidean_distance(user_data, start+1, start)*get_euclidean_distance(user_data, start+1, start+2)\n",
        "  if denominator == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return np.arccos(numerator/denominator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQF6YcKxER7p"
      },
      "source": [
        "def get_curvature_distance(user_data, start, end):\n",
        "  if start+2 > user_data.index[-1]:\n",
        "      return 0\n",
        "  \n",
        "  dx = get_x_distance(user_data, start, start+2)\n",
        "  dy = get_x_distance(user_data, start, start+2)\n",
        "  numerator = dy*user_data[\"px\"][start+1]+dx*user_data[\"py\"][start+1]+(user_data[\"px\"][start]*user_data[\"py\"][start+2]-user_data[\"px\"][start+2]*user_data[\"py\"][start])\n",
        "  distance_xy_2 = np.sqrt(np.square(dx)+np.square(dy))\n",
        "  if distance_xy_2 == 0:\n",
        "    return 0\n",
        "  else: \n",
        "    return numerator/distance_xy_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x-C8rRxHimi"
      },
      "source": [
        "def get_angle(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "      return 0\n",
        "  if end - start < 3:\n",
        "    return 0\n",
        "  numerator = np.square(get_euclidean_distance(user_data, start, user_data.index[0]))+np.square(get_euclidean_distance(user_data, start, user_data.index[-1]))-np.square(get_euclidean_distance(user_data,  user_data.index[0], user_data.index[-1]))\n",
        "  denominator = 2*get_euclidean_distance(user_data,  user_data.index[0], start)*get_euclidean_distance(user_data, start, user_data.index[-1])\n",
        "  if denominator == 0 or numerator/denominator < -1 or numerator/denominator>0:\n",
        "    return 0\n",
        "  else:\n",
        "    return np.arccos(numerator/denominator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKliTPuVHD9k"
      },
      "source": [
        "#not done\n",
        "def get_curve_length_ratio(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  return get_euclidean_distance(user_data, start, end)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9SDXG74Q79W"
      },
      "source": [
        "def get_straightness(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  dist = 0\n",
        "  tot_dist = get_euclidean_distance(user_data, user_data.index[0], user_data.index[-1])\n",
        "  for i in user_data.index[:-1]:\n",
        "    dist += get_euclidean_distance(user_data, i, i+1)\n",
        "  if dist == 0:\n",
        "    return 0\n",
        "  return tot_dist / dist\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf5gHtMESCXk"
      },
      "source": [
        "def get_trajectory_center_of_mass(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  dist = 0\n",
        "  for i in user_data.index[:-1]:\n",
        "    dist += get_euclidean_distance(user_data, i, i+1)\n",
        "  elapsed_time = get_elapsed_time(user_data, start,end)\n",
        "  current_distance = get_euclidean_distance(user_data,start,end)\n",
        "  if dist == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return (current_distance*elapsed_time)/dist\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Meque0eYUtzF"
      },
      "source": [
        "def get_scattering_coefficient(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  dist = 0\n",
        "  for i in user_data.index[:-1]:\n",
        "    dist += get_euclidean_distance(user_data, i, i+1)\n",
        "  TCM = 0\n",
        "  for j in user_data.index[:-1]:\n",
        "    TCM += get_trajectory_center_of_mass(user_data, start, end)\n",
        "  elapsed_time = get_elapsed_time(user_data, start,end)\n",
        "  current_distance = get_euclidean_distance(user_data,start,end)\n",
        "  if dist == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return (current_distance*np.square(elapsed_time)-np.square(TCM))/dist\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TteD4uTbXq2d"
      },
      "source": [
        "def get_third_moment(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  dist = 0\n",
        "  for i in user_data.index[:-1]:\n",
        "    dist += get_euclidean_distance(user_data, i, i+1)\n",
        "  elapsed_time = get_elapsed_time(user_data, start,end)\n",
        "  current_distance = get_euclidean_distance(user_data,start,end)\n",
        "  if dist == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return (current_distance*(elapsed_time**3))/dist\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsTMN1nyXqL1"
      },
      "source": [
        "def get_fourth_moment(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  dist = 0\n",
        "  for i in user_data.index[:-1]:\n",
        "    dist += get_euclidean_distance(user_data, i, i+1)\n",
        "  elapsed_time = get_elapsed_time(user_data, start,end)\n",
        "  current_distance = get_euclidean_distance(user_data,start,end)\n",
        "  if dist == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return (current_distance*(elapsed_time**4))/dist\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HYqr5XKYK04"
      },
      "source": [
        "#??????????????\n",
        "def get_trajectory_curvature(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "    return 0\n",
        "  dist = 0\n",
        "  for i in user_data.index[:-1]:\n",
        "    dist += get_euclidean_distance(user_data, i, i+1)\n",
        "  elapsed_time = get_elapsed_time(user_data, start,end)\n",
        "  current_distance = get_euclidean_distance(user_data,start,end)\n",
        "  return (current_distance*(elapsed_time**3))/dist\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIRiBc3p-Yma"
      },
      "source": [
        "\n",
        "def get_deviation(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "      return 0\n",
        "  dx = get_x_distance(user_data, user_data.index[0], user_data.index[-1])\n",
        "  dy = get_y_distance(user_data, user_data.index[0], user_data.index[-1])\n",
        "  \n",
        "  numerator = dy*user_data[\"px\"][start]+dx*user_data[\"py\"][start]+(user_data[\"px\"][user_data.index[0]]*user_data[\"py\"][user_data.index[1]]-user_data[\"px\"][user_data.index[1]]*user_data[\"py\"][user_data.index[0]])\n",
        "  distance_xy = np.sqrt(np.square(dx)+np.square(dy))\n",
        "  if distance_xy == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return numerator/distance_xy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YypKhBzM9D5H"
      },
      "source": [
        "def get_velocity_curvature(user_data, start, end):\n",
        "  if end > user_data.index[-1]:\n",
        "      return 0\n",
        "  jerk = get_jerk(user_data, start, end)\n",
        "  acceleration = get_acceleration(user_data, start, end)\n",
        "  if acceleration == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return jerk/((1+acceleration**2)**(3/2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHjgQqOVa-8n"
      },
      "source": [
        "# wrapper for feature functions\n",
        "class feature:\n",
        "  def __init__(self, feature_func, return_func, offset=1):\n",
        "    self.feature_func = feature_func\n",
        "    self.return_func = return_func\n",
        "    self.offset = offset\n",
        "  \n",
        "  def get_feature(self, user_data):\n",
        "    flist = []\n",
        "    if self.offset is None:\n",
        "      flist.append(self.feature_func(user_data, user_data.index[0], user_data.index[-1]))\n",
        "    else:\n",
        "      for i in user_data.index:\n",
        "        f = self.feature_func(user_data, i, i+self.offset)\n",
        "        if np.isnan(f):\n",
        "          continue\n",
        "        flist.append(f)\n",
        "\n",
        "    return self.return_func(flist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvcoXa_v9wL5"
      },
      "source": [
        "Custom return functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1JqC0lKADJQ"
      },
      "source": [
        "def nz_max(input_array):\n",
        "  new_array = [i for i in input_array if i != 0]\n",
        "  if len(new_array) == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return np.max(new_array)\n",
        "\n",
        "def nz_min(input_array):\n",
        "  new_array = [i for i in input_array if i != 0]\n",
        "  if len(new_array) == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return np.min(new_array)\n",
        "\n",
        "def nz_range(input_array):\n",
        "  return nz_max(input_array)-nz_min(input_array)\n",
        "\n",
        "def nz_std(input_array):\n",
        "  new_array = [i for i in input_array if i != 0]\n",
        "  if len(new_array) == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return np.std(new_array)\n",
        "\n",
        "\n",
        "def nz_mean(input_array):\n",
        "  new_array = [i for i in input_array if i != 0]\n",
        "  if len(new_array) == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return np.mean(new_array)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2OeGkUGs-Ca"
      },
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDVOdsuhG9OF"
      },
      "source": [
        "def extract_features(train_path, test_path, data, user_ids, feature_functions, test_size=0.3, min_data=2000):\n",
        "\n",
        "  window_length = 5000  # delta_time\n",
        "  window_offset = None  # offset_time\n",
        "\n",
        "  cols = list([\"user_id\"]) + list(feature_functions.keys())\n",
        "  train_df = pd.DataFrame(columns=cols)\n",
        "  test_df = pd.DataFrame(columns=cols)\n",
        "  res = \"y\"\n",
        "  if (os.path.exists(train_path) or os.path.exists(test_path)):\n",
        "    res = input(\"Do you want to overwrite feature data?\\n[y][n]: \")\n",
        "  if res == \"y\":\n",
        "    for user in user_ids: \n",
        "      if len(train_df.index)==0 and os.path.exists(train_path):\n",
        "        exist = False\n",
        "        with open(train_path, 'rt') as f:\n",
        "          s = csv.reader(f, delimiter=\",\")\n",
        "          for row in s:\n",
        "            if str(user) in row[0]:\n",
        "              exist = True\n",
        "              continue\n",
        "        if exist == True:\n",
        "          continue\n",
        "      if user != 0.0:\n",
        "        user_data = data[data[\"user_id\"] == user]\n",
        "        if len(user_data.index) < min_data:\n",
        "          print(\"Skipping user: {}\".format(user), len(user_data.index), len(user_data))\n",
        "          continue\n",
        "        print(\"Extracting features from user: {}\".format(user))\n",
        "        # init feature dict\n",
        "        user_feature = {key : [] for key in cols}\n",
        "\n",
        "        # extract_features\n",
        "        for window in time_window(user_data, window_length, window_offset):\n",
        "          user_feature[\"user_id\"].append(user)\n",
        "          for key in feature_functions:\n",
        "            f = feature_functions[key].get_feature(window)\n",
        "            if np.isnan([f]): \n",
        "              print(key, f)\n",
        "            user_feature[key].append(f)\n",
        "        # clear_output()\n",
        "\n",
        "        # split training and testing\n",
        "        user_train, user_test = train_test_split(pd.DataFrame.from_dict(user_feature), test_size=0.3, shuffle=False)\n",
        "        train_df = pd.concat([train_df, user_train])\n",
        "        test_df = pd.concat([test_df, user_test])\n",
        "        if user == user_ids[0]:\n",
        "          train_df.to_csv(train_path, index=False)\n",
        "          test_df.to_csv(test_path, index=False)\n",
        "        else:\n",
        "          train_df = train_df.reset_index(drop = True)\n",
        "          test_df = test_df.reset_index(drop = True)\n",
        "      \n",
        "          with open(train_path, 'a') as f:\n",
        "            train_df.to_csv(f, header=False, index=False)\n",
        "          with open(test_path, 'a') as f:\n",
        "            test_df.to_csv(f, header=False, index=False)\n",
        "    \n",
        "  else:\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "  \n",
        "  return train_df, test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tApnfmRTE_XZ"
      },
      "source": [
        "# dict of functions to call\n",
        "feature_functions = {\n",
        "  \"elapsed_time\": feature(get_elapsed_time, np.sum),\n",
        "  \"critical_points\": feature(get_critical_points, np.sum),\n",
        "  \"stroke-length\": feature(get_euclidean_distance, np.sum),\n",
        "  \"straightness\": feature(get_straightness, nz_max),\n",
        "  \"trajectory_center_of_mass\": feature(get_trajectory_center_of_mass, np.sum),\n",
        "  #\"scattering_coefficient\": feature(get_scattering_coefficient, np.sum),\n",
        "  \"third_moment\": feature(get_third_moment, np.sum),\n",
        "  \"fourth_moment\": feature(get_fourth_moment, np.sum),\n",
        "  \"velocity_curvature\": feature(get_velocity_curvature, nz_mean, 4),\n",
        " \n",
        "  \"xvelocity-mean\": feature(get_x_velocity, nz_mean),\n",
        "  \"xvelocity-maximum\": feature(get_x_velocity, nz_max), \n",
        "  \"xvelocity-minimum\": feature(get_x_velocity, nz_min), \n",
        "  \"xvelocity-std\": feature(get_x_velocity, nz_std), \n",
        "  \"xvelocity-range\": feature(get_x_velocity, nz_range),\n",
        "  \"yvelocity-mean\": feature(get_y_velocity, nz_mean),\n",
        "  \"yvelocity-maximum\": feature(get_y_velocity, nz_max), \n",
        "  \"yvelocity-minimum\": feature(get_y_velocity, nz_min), \n",
        "  \"yvelocity-std\": feature(get_y_velocity, nz_std), \n",
        "  \"yvelocity-range\": feature(get_y_velocity, nz_range),\n",
        "  \"tangential-velocity-mean\": feature(get_speed, nz_mean),\n",
        "  \"tangential-velocity-maximum\": feature(get_speed, nz_max), \n",
        "  \"tangential-velocity-minimum\": feature(get_speed, nz_min), \n",
        "  \"tangential-velocity-std\": feature(get_speed, nz_std), \n",
        "  \"tangential-velocity-range\": feature(get_speed, nz_range),\n",
        "  \"acceleration-mean\": feature(get_acceleration, nz_mean,3),\n",
        "  \"acceleration-maximum\": feature(get_acceleration, nz_max,3), \n",
        "  \"acceleration-minimum\": feature(get_acceleration, nz_min,3), \n",
        "  \"acceleration-std\": feature(get_acceleration, nz_std,3), \n",
        "  \"acceleration-range\": feature(get_acceleration, nz_range,3), \n",
        "  \"jerk-mean\": feature(get_jerk, nz_mean, 4),\n",
        "  \"jerk-maximum\": feature(get_jerk, nz_max, 4), \n",
        "  \"jerk-minimum\": feature(get_jerk, nz_min, 4), \n",
        "  \"jerk-std\": feature(get_jerk, nz_std, 4), \n",
        "  \"jerk-range\": feature(get_jerk, nz_range, 4),  \n",
        "  \"angular_velocity-mean\": feature(get_angular_velocity, nz_mean),\n",
        "  \"angular_velocity-maximum\": feature(get_angular_velocity, nz_max), \n",
        "  \"angular_velocity-minimum\": feature(get_angular_velocity, nz_min), \n",
        "  \"angular_velocity-std\": feature(get_angular_velocity, nz_std), \n",
        "  \"angular_velocity-range\": feature(get_angular_velocity, nz_range),\n",
        "  \"curvature-mean\": feature(get_curvature, nz_mean),\n",
        "  \"curvature-maximum\": feature(get_curvature, nz_max), \n",
        "  \"curvature-minimum\": feature(get_curvature, nz_min), \n",
        "  \"curvature-std\": feature(get_curvature, nz_std), \n",
        "  \"curvature-range\": feature(get_curvature, nz_range),\n",
        "  \"curvature_change-mean\": feature(get_curvature_change, nz_mean,3),\n",
        "  \"curvature_change-maximum\": feature(get_curvature_change, nz_max,3), \n",
        "  \"curvature_change-minimum\": feature(get_curvature_change, nz_min,3), \n",
        "  \"curvature_change-std\": feature(get_curvature_change, nz_std,3), \n",
        "  \"curvature_change-range\": feature(get_curvature_change, nz_range,3),\n",
        "  \"direction-mean\": feature(get_direction, nz_mean),\n",
        "  \"direction-maximum\": feature(get_direction, nz_max), \n",
        "  \"direction-minimum\": feature(get_direction, nz_min), \n",
        "  \"direction-std\": feature(get_direction, nz_std), \n",
        "  \"direction-range\": feature(get_direction, nz_range),\n",
        "  \"angle-mean\": feature(get_angle, nz_mean,3),\n",
        "  \"angle-maximum\": feature(get_angle, nz_max,3), \n",
        "  \"angle-minimum\": feature(get_angle, nz_min,3), \n",
        "  \"angle-std\": feature(get_angle, nz_std,3), \n",
        "  \"angle-range\": feature(get_angle, nz_range,3),\n",
        "  \"curvature_distance-mean\": feature(get_curvature_distance, nz_mean),\n",
        "  \"curvature_distance-maximum\": feature(get_curvature_distance, nz_max), \n",
        "  \"curvature_distance-minimum\": feature(get_curvature_distance, nz_min), \n",
        "  \"curvature_distance-std\": feature(get_curvature_distance, nz_std), \n",
        "  \"curvature_distance-range\": feature(get_curvature_distance, nz_range),\n",
        "  \"deviation-mean\": feature(get_deviation, nz_mean),\n",
        "  \"deviation-maximum\": feature(get_deviation, nz_max), \n",
        "  \"deviation-minimum\": feature(get_deviation, nz_min), \n",
        "  \"deviation-std\": feature(get_deviation, nz_std), \n",
        "  \"deviation-range\": feature(get_deviation, nz_range)\n",
        "  # \"angle_of_curvature-mean\": feature(get_angle_of_curvature, nz_mean),\n",
        "  # \"angle_of_curvature-maximum\": feature(get_angle_of_curvature, nz_max), \n",
        "  # \"angle_of_curvature-minimum\": feature(get_angle_of_curvature, nz_min), \n",
        "  # \"angle_of_curvature_curvature-std\": feature(get_angle_of_curvature, nz_std), \n",
        "  # \"angle_of_curvature-range\": feature(get_angle_of_curvature, nz_range),  \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2ptarioZG9a",
        "outputId": "0d8763b8-be57-4861-8e9f-ae802d930683"
      },
      "source": [
        "#   # bb-mas features\n",
        "# bb_mas_train_df, bb_mas_test_df = extract_features(\n",
        "#    \"Data/Features/Training_BB-MAS_Features.csv\",\n",
        "#    \"Data/Features/Testing_BB-MAS_Features.csv\",\n",
        "#     bb_mas_df, bb_mas_df[\"user_id\"].unique(), \n",
        "#    feature_functions, 0.3\n",
        "# )\n",
        "\n",
        "# # # twos features\n",
        "# twos_train_df, twos_test_df = extract_features(\n",
        "#  \"Data/Features/Training_TWOS_Features.csv\",\n",
        "#  \"Data/Features/Testing_TWOS_Features.csv\",\n",
        "#  twos_df, twos_df[\"user_id\"].unique(), \n",
        "#  feature_functions, 0.3\n",
        "# )\n",
        "\n",
        "balabit_train_df, balabit_test_df = extract_features(\n",
        " \"Data/Features/Training_Balabit_Features.csv\",\n",
        " \"Data/Features/Testing_Balabit_Features.csv\",\n",
        " balabit_df, balabit_df[\"user_id\"].unique(), \n",
        " feature_functions, 0.3\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you want to overwrite feature data?\n",
            "[y][n]: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwryBSDH1uWw"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGDdM6BpjY8k"
      },
      "source": [
        "A utility method to create a tf.data dataset from a Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlC42uVZja_K"
      },
      "source": [
        "# def df_to_dataset(dataframe, y_label, shuffle=True, batch_size=32):\n",
        "#   dataframe = dataframe.copy() \n",
        "#   labels = dataframe.pop(y_label)\n",
        "#   ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "#   if shuffle:\n",
        "#     ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "#   ds = ds.batch(batch_size)\n",
        "#   return ds\n",
        "\n",
        "def df_to_dataset(X, y, shuffle=True, batch_size=32):\n",
        "  ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ6xOQrIQHyt"
      },
      "source": [
        "# get X, y dataframes\n",
        "def prep_feature_df(user_data, user_id):\n",
        "  y = [ [int(user_id == i)] for i in user_data[\"user_id\"]]\n",
        "  X = user_data.copy()\n",
        "  X.pop(\"user_id\")\n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlKJLAYPAxJh"
      },
      "source": [
        "## ML Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KSg179gfBd6"
      },
      "source": [
        "#hyperparamater optimization\n",
        "\n",
        "def LogisticRegression_optimize(X, y, path, model):\n",
        "  solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "  penalty = ['none', 'l1', 'l2', 'elasticnet']\n",
        "  c_values = [100, 10, 1.0, 0.1, 0.01]\n",
        "  # define grid search\n",
        "  grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)\n",
        "  grid_result = grid_search.fit(X, y)\n",
        "  # summarize results\n",
        "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "  means = grid_result.cv_results_['mean_test_score']\n",
        "  stds = grid_result.cv_results_['std_test_score']\n",
        "  params = grid_result.cv_results_['params']\n",
        "  for mean, stdev, param in zip(means, stds, params):\n",
        "      print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "  params['score'] = means\n",
        "  pd.DataFrame.from_dict(params).to_csv(path + \"/Best_Params_LR.csv\")\n",
        "\n",
        "\n",
        "def KNN_optimize(X, y, path, model):\n",
        "  n_neighbors = range(1, 21, 2)\n",
        "  #weights = ['uniform', 'distance']\n",
        "  #metric = ['euclidean', 'manhattan', 'minkowski', 'mahalanobis']\n",
        "  weights = ['distance']\n",
        "  metric = ['mahalanobis']\n",
        "  # define grid search\n",
        "  grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)\n",
        "  grid_result = grid_search.fit(X, y)\n",
        "  # summarize results\n",
        "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "  means = grid_result.cv_results_['mean_test_score']\n",
        "  stds = grid_result.cv_results_['std_test_score']\n",
        "  params = grid_result.cv_results_['params']\n",
        "  for mean, stdev, param in zip(means, stds, params):\n",
        "      print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "  params['score'] = means\n",
        "  pd.DataFrame.from_dict(params).to_csv(path + \"/Best_Params_KNN.csv\")\n",
        "\n",
        "def SVM_optimize(X, y, path, model):\n",
        "  kernel = ['poly', 'rbf', 'sigmoid']\n",
        "  C = [100, 10, 1.0, 0.1, 0.01]\n",
        "  gamma = ['scale']\n",
        "  # define grid search\n",
        "  grid = dict(kernel=kernel,C=C,gamma=gamma)\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)\n",
        "  grid_result = grid_search.fit(X, y)\n",
        "  # summarize results\n",
        "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "  means = grid_result.cv_results_['mean_test_score']\n",
        "  stds = grid_result.cv_results_['std_test_score']\n",
        "  params = grid_result.cv_results_['params']\n",
        "  for mean, stdev, param in zip(means, stds, params):\n",
        "      print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "  params['score'] = means\n",
        "  pd.DataFrame.from_dict(params).to_csv(path + \"/Best_Params_SVM.csv\")\n",
        "\n",
        "def RandomForest_optimize(X, y, path, model):\n",
        "  n_estimators = [10, 100, 1000]\n",
        "  max_features = ['sqrt', 'log2']\n",
        "  # define grid search\n",
        "  grid = dict(n_estimators=n_estimators,max_features=max_features)\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)\n",
        "  grid_result = grid_search.fit(X, y)\n",
        "  # summarize results\n",
        "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "  means = grid_result.cv_results_['mean_test_score']\n",
        "  stds = grid_result.cv_results_['std_test_score']\n",
        "  params = grid_result.cv_results_['params']\n",
        "  for mean, stdev, param in zip(means, stds, params):\n",
        "      print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "  params['score'] = means\n",
        "  pd.DataFrame.from_dict(params).to_csv(path + \"/Best_Params_RF.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPNEZlDljQi6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFxYOKWQgOTL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNmv9bdPf6Wi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OAnZyxUDkJa",
        "outputId": "2fe7ed44-37e9-43bc-d8a6-706461d81078"
      },
      "source": [
        "# models = {\n",
        "#   \"SVM\": svm.SVC()\n",
        "#   \"KNN-7\": KNeighborsClassifier(n_neighbors=7),\n",
        "#   \"KNN-9\": KNeighborsClassifier(n_neighbors=9),\n",
        "#   \"Random Forest\": RandomForestClassifier(max_depth=2, random_state=0),\n",
        "#   \"Random Forest2\": RandomForestClassifier(random_state=1),\n",
        "#   \"KNN-5\": KNeighborsClassifier(n_neighbors=5),\n",
        "#   \"KNN-3\": KNeighborsClassifier(n_neighbors=3),\n",
        "#   \"LR\": LogisticRegression(random_state=0),\n",
        "#   \"GNB\": GaussianNB()\n",
        "# }\n",
        "\n",
        "models = {\n",
        "  #\"SVM\": svm.SVC(),\n",
        "  \"KNN\": KNeighborsClassifier(),\n",
        "  # \"Random Forest\": RandomForestClassifier(),\n",
        "  # \"LR\": LogisticRegression()\n",
        "}\n",
        "user_id = balabit_df[\"user_id\"].unique()\n",
        "accuracy = {key: [] for key in [\"user_id\"] + list(models.keys())}\n",
        "k_best = {}\n",
        "train_df = balabit_train_df\n",
        "#print(user_id)\n",
        "random.seed(12345)\n",
        "sample_users = balabit_train_df[\"user_id\"].unique()\n",
        "sample_users = [7]\n",
        "#print(\"Sampled Users: \", sample_users)\n",
        "for user in sample_users:\n",
        "  print(\"User {}\".format(user))\n",
        "  X_train, y_train = prep_feature_df(train_df, user)\n",
        "\n",
        "  # make sure there are enough true users\n",
        "  if (np.sum(y_train) < 10):\n",
        "    continue\n",
        "  # oversample classes\n",
        "  sm = SMOTE(sampling_strategy=\"minority\", random_state=0)\n",
        "  X_res, y_train = sm.fit_resample(X_train, y_train)\n",
        "  X_train = pd.DataFrame(X_res, columns=X_train.columns)\n",
        "\n",
        "  # make feature profile\n",
        "  selector = SelectNonCollinear(0.8)\n",
        "  selector.fit(np.array(X_train), np.array(y_train))\n",
        "  mask = selector.get_support()\n",
        "  X_train = X_train[X_train.columns[mask]]\n",
        "\n",
        "  fs = SelectPercentile(percentile=50)\n",
        "  fs.fit_transform(X_train, y_train)\n",
        "  mask = fs.get_support()\n",
        "  X_train = X_train[X_train.columns[mask]]\n",
        "\n",
        "  k_best[user] = X_train.columns\n",
        "\n",
        "  # print(k_best)\n",
        "\n",
        "  cv = 5\n",
        "  #for i in range(cv):\n",
        "    #accuracy[\"user_id\"].append(user)\n",
        "  path = \"Models/Balabit/{}\".format(user)\n",
        "  if os.path.isdir(path) == False:\n",
        "    print(\"new path\")\n",
        "    os.makedirs(path)\n",
        "  # if os.path.isdir(path) == False:\n",
        "  #   print(\"new path\")\n",
        "  #   os.mkdir(path)\n",
        "  # path2 = \"Features/BB-MAS/{}\".format(user)\n",
        "  # os.mkdir(path2)\n",
        "  # with open(\"Features/BB-MAS/{}.pkl\".format(user), 'wb') as f:\n",
        "  #     pickle.dump(,f)\n",
        "\n",
        "\n",
        "  for key in models:\n",
        "    print(key)\n",
        "    # models[key].fit(X_train, y_train)\n",
        "    # # accuracy[key].append(models[key].score(X_train, y_train))\n",
        "    # score = cross_val_score(models[key], X_train, y_train, cv=5, scoring = \"f1\")\n",
        "    # #score = models[key].score(X_train, y_train)\n",
        "    # #for i in score:\n",
        "    # accuracy[key].append(score)\n",
        "  #   file = Path(path + \"/{}.pkl\".format(key))\n",
        "  #   if (file.is_file() == False):\n",
        "  #     with open(path + \"/{}.pkl\".format(key), 'wb') as f:\n",
        "  #           pickle.dump(models[key],f)\n",
        "  # if Path(\"Models/BB-MAS/{}/features.csv\".format(user)).is_file() == False:\n",
        "  #   pd.DataFrame.from_dict(k_best[user]).to_csv(\"Models/BB-MAS/{}/features.csv\".format(user))\n",
        "    if key == \"SVM\":\n",
        "      SVM_optimize(X_train, y_train, path, models[key])\n",
        "    elif key == \"KNN\":\n",
        "      KNN_optimize(X_train, y_train, path, models[key])\n",
        "    elif key == \"RandomForest\":\n",
        "      RandomForest_optimize(X_train, y_train, path, models[key])\n",
        "    elif key == \"LR\":\n",
        "      LogisticRegression_optimize(X_train, y_train, path, models[key])\n",
        "  clear_output()\n",
        "#print(accuracy)\n",
        "#del accuracy['user_id']\n",
        "\n",
        "#pd.DataFrame.from_dict(accuracy).to_csv(\"BB-MAS-training-f1-accuracy.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User 7\n",
            "KNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIrvqMHJ3MXW"
      },
      "source": [
        "\n",
        "pd.DataFrame.from_dict(accuracy).to_csv(\"BB-MAS-training-balanced-accuracy.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S5EdxffUVQd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVlGMKenKHhf"
      },
      "source": [
        "test_df = bb_mas_test_df\n",
        "#sample_users = train_df[\"user_id\"].unique()\n",
        "sample_users = [7]\n",
        "\n",
        "eval_accuracy = {key: [] for key in [\"user_id\"] + list(models.keys())}\n",
        "eval_auc = {key: [] for key in [\"user_id\"] + list(models.keys())}\n",
        "eval_far = {key: [] for key in [\"user_id\"] + list(models.keys())}\n",
        "eval_frr = {key: [] for key in [\"user_id\"] + list(models.keys())}\n",
        "eval_eer = {key: [] for key in [\"user_id\"] + list(models.keys())}\n",
        "eval_hter = {key: [] for key in [\"user_id\"] + list(models.keys())}\n",
        "\n",
        "for user in sample_users:\n",
        "    print(user)\n",
        "    model_path = \"Models/Balabit/{}\".format(user)\n",
        "    if os.path.isdir(model_path) == True:\n",
        "      features = pd.read_csv(\"Models/Balabit/{}\".format(user) +'/features.csv')\n",
        "      X_test, y_test = prep_feature_df(test_df, user)\n",
        "      X_test = X_test[features[\"0\"]]\n",
        "\n",
        "      for key in models:\n",
        "        print(key)\n",
        "        with open(model_path + \"/{}.pkl\".format(key), 'rb') as f:\n",
        "              loaded_model = pickle.load(f)\n",
        "        predicted_y = loaded_model.predict(X_test)\n",
        "        tn, fp, fn, tp = confusion_matrix(y_test, predicted_y).ravel()\n",
        "        far = fp / (fp + tn)\n",
        "        frr = fn / (fn + tp)\n",
        "        hter = (far + frr) / 2\n",
        "\n",
        "        # score = loaded_model.score(X_test, y_test)\n",
        "        # auc = roc_auc_score(y_test, loaded_model.predict_proba(X_test)[:,1])\n",
        "        # far, tpr, threshold = roc_curve(y_test, loaded_model.predict_proba(X_test)[:,1], pos_label=1)\n",
        "        # frr = 1 - tpr\n",
        "        # EER = far[np.nanargmin(np.absolute((frr - far)))]\n",
        "        eval_hter[key].append(hter)\n",
        "        # eval_auc[key].append(auc)\n",
        "        eval_far[key].append(far)\n",
        "        eval_frr[key].append(frr)\n",
        "        \n",
        "        # eval_eer[key].append(EER)\n",
        "        # print(\"score: \", score)\n",
        "        # print(\"auc: \", auc)\n",
        "        print(\"far: \", far)\n",
        "        print(\"frr: \", frr)\n",
        "        print(\"hter: \", hter)\n",
        "      \n",
        "        \n",
        "\n",
        "\n",
        "pd.DataFrame.from_dict(eval_hter).to_csv(\"Balabit_test_hter.csv\")\n",
        "# pd.DataFrame.from_dict(eval_auc).to_csv(\"BB-MAS_test_AUC.csv\")\n",
        "pd.DataFrame.from_dict(eval_far).to_csv(\"Balabit_test_far.csv\")\n",
        "pd.DataFrame.from_dict(eval_frr).to_csv(\"Balabit_test_frr.csv\")\n",
        "#pd.DataFrame.from_dict(eval_eer).to_csv(\"BB-MAS_test_eer.csv\")\n",
        "print(eval_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6sHvaPsegLQ"
      },
      "source": [
        "print(\"accuracy: \", eval_accuracy)\n",
        "print(\"auc: \", eval_auc)\n",
        "print(\"eer: \", eval_eer)\n",
        "# print(\"far: \", eval_far)\n",
        "# print(\"frr: \", eval_frr)\n",
        "\n",
        "eval_accuracy['user_id'] = sample_users\n",
        "eval_auc['user_id'] = sample_users\n",
        "eval_eer['user_id'] = sample_users\n",
        "eval_far['user_id'] = sample_users\n",
        "eval_frr['user_id'] = sample_users\n",
        "print(eval_accuracy)\n",
        "\n",
        "pd.DataFrame.from_dict(eval_accuracy).to_csv(\"BB-MAS_test_accuracy.csv\")\n",
        "pd.DataFrame.from_dict(eval_auc).to_csv(\"BB-MAS_test_AUC.csv\")\n",
        "pd.DataFrame.from_dict(eval_far).to_csv(\"BB-MAS_test_far.csv\")\n",
        "pd.DataFrame.from_dict(eval_frr).to_csv(\"BB-MAS_test_frr.csv\")\n",
        "pd.DataFrame.from_dict(eval_eer).to_csv(\"BB-MAS_test_eer.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfVBa-RlGGII"
      },
      "source": [
        "\n",
        "print(eval_accuracy['LR'])\n",
        "print(eval_accuracy['GNB'])\n",
        "print(eval_accuracy['Random Forest'])\n",
        "print((eval_accuracy['Random Forest2']))\n",
        "#print(len(eval_accuracy['LR']))\n",
        "#print(len(eval_accuracy['LR']))\n",
        "#print(len(eval_accuracy['LR']))\n",
        "##del eval_accuracy['user_id']\n",
        "#pd.DataFrame.from_dict(eval_accuracy).to_csv(\"test_eval_accuracy.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zxzhkSFG1gV"
      },
      "source": [
        "features = pd.read_csv(\"test_eval_accuracy.csv\")\n",
        "x = list(range(0,99))\n",
        "\n",
        "plt.xlabel(\"User\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.figure(1)\n",
        "y = features['Random Forest']\n",
        "plt.bar(x,y)\n",
        "plt.title('Random Forest Test Accuracy')\n",
        "plt.figure(2)\n",
        "y = features['Random Forest2']\n",
        "plt.bar(x,y)\n",
        "plt.title('Random Forest2 Test Accuracy')\n",
        "plt.figure(3)\n",
        "y = features['KNN-5']\n",
        "plt.bar(x,y)\n",
        "plt.title('KNN-5 Test Accuracy')\n",
        "plt.figure(4)\n",
        "y = features['KNN-3']\n",
        "plt.bar(x,y)\n",
        "plt.title('KNN-3 Test Accuracy')\n",
        "plt.figure(5)\n",
        "y = features['LR']\n",
        "plt.bar(x,y)\n",
        "plt.title('LR Test Accuracy')\n",
        "plt.figure(6)\n",
        "y = features['GNB']\n",
        "plt.bar(x,y)\n",
        "plt.title('GNB Test Accuracy')\n",
        "plt.figure(7)\n",
        "y = features['SVM']\n",
        "plt.bar(x,y)\n",
        "plt.title('SVM Test Accuracy')\n",
        "plt.figure(8)\n",
        "y = features['KNN-7']\n",
        "plt.bar(x,y)\n",
        "plt.title('KNN-7 Test Accuracy')\n",
        "plt.figure(9)\n",
        "y = features['KNN-9']\n",
        "plt.bar(x,y)\n",
        "plt.title('KNN-9 Test Accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdsLIdzMxu18"
      },
      "source": [
        "for key in accuracy.keys():\n",
        "  \n",
        "  print(len(accuracy[key]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MZTLA4zyw-k"
      },
      "source": [
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNi5vPX2IGFM"
      },
      "source": [
        "acc_df = pd.DataFrame.from_dict(accuracy)\n",
        "# acc_df.to_csv(\"\")\n",
        "print(acc_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}